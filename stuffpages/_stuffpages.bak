#!/usr/bin/env python


import os
import re
import shutil
import codecs
import fnmatch
import time
import json
from datetime import datetime
from glob import glob
import sys; sys.dont_write_bytecode = True

from markdown import Markdown
from markdown.extensions.meta import MetaPreprocessor
from bs4 import BeautifulSoup


def split_meta(filename):
        """Split markdown file into meta data, meta text and main text."""

        with codecs.open(filename, encoding='utf-8') as f:
            text = f.read().split("\n")
            class FakeMeta:
                def __init__(self):
                    self.Meta = None
            class MetaPreprocessor2(MetaPreprocessor):
                def __init__(self):
                    self.md = FakeMeta()
            m = MetaPreprocessor2()
            main_text = m.run(text[:])
            meta_text = [x for x in set(text).difference(set(main_text)) \
                         if x not in ["---", "..."]]
        return (m.md.Meta, "\n".join(meta_text).strip(),
                "\n".join(main_text).strip())


class StuffPages:
    """A class representing a StuffPages directory."""

    def __init__(self, markdown_dir):
        """Initialise a StuffPages object.

        Parameters
        ----------
        markdown_dir : str
            the directory with the .md files

        """

        self.markdown_dir = markdown_dir

    def init(self, style=None):
        """Create a default 'config.py'."""

        #TODO: ask if config found
        config_path = os.path.abspath(os.path.split(__file__)[0])
        config_file = os.path.join(config_path, "config.py")
        try:
            shutil.copy(config_file, os.path.join(self.markdown_dir,
                                                  "config.py"))
            shutil.copytree(os.path.join(config_path, "styles"),
                            os.path.join(self.markdown_dir, "styles"))
        except:
            pass

    def build(self):
        """Build all pages ('*.md')."""

        #TODO: ask if lastupdate is found
        try:
            os.remove(os.path.join(self.markdown_dir, ".lastupdate"))
        except:
            pass
        self.update()

    def update(self):
        """Update new/changed pages."""

        # Load global config
        from .config import extras, extras_configs, defaults
        from .config import html_head, html_nav, html_header, html_footer
        from .config import pagelisting_format

        # Load local config
        try:  # TODO: adapt for Py3
            imp.load_module(imp.find_module("config", [self.markdown_dir]))
            extras = config.extras
            extras_configs = config.extras
            defaults = config.defaults
            html_head = config.html_head
            html_nav = config.html_nav
            html_header = config.html_header
            html_footer = config.html_footer
            pagelisting_format = config.pagelisting_format

        except:
            pass

        # Load update information
        if os.path.exists(os.path.join(self.markdown_dir, ".lastupdate")):
            with open(os.path.join(self.markdown_dir, ".lastupdate")) as f:
                lastupdate = json.load(f)
        else:
            lastupdate = {"time": 0,
                          "pagelisting_files": []}

        # Loop (recursively) over all markdown files in directory
        outfiles = []
        matches = []
        written_pages = []
        written_files = {}

        for root, dirnames, filenames in os.walk(self.markdown_dir):
            for filename in fnmatch.filter(filenames, '*.md'):
                if os.path.relpath(root,
                                   defaults["output_dir"]).startswith(".."):
                    matches.append(os.path.join(root, filename))

        for filename in matches + list(set(lastupdate["pagelisting_files"]) \
                - set(matches)):
            if not filename in lastupdate["pagelisting_files"]:
                if os.path.getmtime(filename) < lastupdate["time"]:
                    continue

            # Read in main content and meta data
            _metas = defaults.copy()
            root, ext = os.path.splitext(filename)
            if "norecursion" in _metas["settings"]:
                if os.path.split(root)[0] != self.markdown_dir:
                    continue
            meta_data, meta_text, main_text = split_meta(filename)

            # Handle meta data
            for m in meta_data.keys():
                _metas[m] = " ".join(meta_data[m])
            if os.path.isabs(_metas["output_dir"]):
                output_dir = _metas["output_dir"]
            else:
                output_dir = os.path.abspath(
                    os.path.join(self.markdown_dir, _metas["output_dir"]))
            if "norecursion" in _metas["settings"]:
                htmldir = os.path.join(output_dir, os.path.split(root)[-1])
                if os.path.split(filename)[-1] == "index.md":
                    htmldir = os.path.split(htmldir)[0]
            else:
                htmldir = os.path.join(output_dir, os.path.relpath(
                    root, self.markdown_dir))
                if os.path.split(filename)[-1] == "index.md":
                    htmldir = os.path.split(htmldir)[0]

            outfile = os.path.join(htmldir, "index.html")
            if not os.path.exists(htmldir):
                os.makedirs(htmldir)

            written_files[outfile] = {'md': os.path.abspath(filename),
                                      'metas': _metas,
                                      'pagelisting': False,
                                      'breadcrumb': False}

            # Handle head, header and footer
            def create_section(section):
                rtn = ''
                for line in [x for x in section if x != ""]:
                    rtn += line + "\n"
                pattern = re.compile(r"{{(.*?)}}")
                for match in pattern.findall(rtn):
                    if match.lower() in _metas:
                        rtn = rtn.replace("{{" + match + "}}",
                                          _metas[match.lower()])
                return rtn.rstrip("\n")

            head = create_section(html_head)

            nav = ''
            if not "nonav" in _metas["settings"]:
                nav = create_section(html_nav)

            header = ''
            if not "noheader" in _metas["settings"]:
                header = create_section(html_header)

            footer = ''
            if not "nofooter" in _metas["settings"]:
                footer = create_section(html_footer)

            # Put everything together
            content = \
    u"""{0}

<!DOCTYPE html>
<html>
{1}
<body>
{2}
<section>

{3}

</section>
{4}
</body>
</html>""".format(meta_text, head, header, main_text, footer)

            # Convert to HTML
            md = Markdown(extensions=['markdown.extensions.meta'] + extras,
                          extension_configs=extras_configs,
                          output_format="html5")
            html = md.convert(content)
            if html.endswith("</p>"):
                html = html[:-4]
            #print(html)
            #sys.exit()

            # Handle linked files
            soup = BeautifulSoup(html, "html.parser")
            links = soup.find_all('link') + \
                    soup.find_all('a') + \
                    soup.find_all('img') + \
                    soup.find_all('source')
            for l in links:
                if l.name in ("link", "a"):
                    tag = "href"
                elif l.name in ("img", "source"):
                    tag = "src"
                link = l[tag]

                # links to generated pages
                if True in [os.path.relpath(
                    os.path.split(m)[0],
                    self.markdown_dir) == link for m in matches]:
                    l[tag] = os.path.join(os.path.relpath(
                        output_dir,
                        os.path.split(outfile)[0]), link)

                # absolute links
                elif os.path.isabs(link):
                    to_ = os.path.join(output_dir, "resources",
                                       os.path.split(link)[-1])
                    if os.path.exists(link):
                        if os.path.isfile(link):
                            if not os.path.exists(os.path.split(to_)[0]):
                                os.makedirs(os.path.split(to_)[0])
                            if not os.path.exists(to_):
                                shutil.copy(link, to_)
                        else:
                            if not os.path.exists(to_):
                                shutil.copytree(link, to_)
                        l[tag] = os.path.join(output_dir, "resources",
                                              split(link)[-1])

                # relative links
                elif not (link.startswith("#") or ":" in link):
                    to_ = os.path.join(output_dir, "resources", link)
                    if os.path.exists(link):
                        if os.path.isfile(link):
                            if not os.path.exists(os.path.split(to_)[0]):
                                os.makedirs(os.path.split(to_)[0])
                            if not os.path.exists(to_):
                                shutil.copy(link, to_)
                        else:
                            if not os.path.exists(to_):
                                shutil.copytree(link, to_)
                        l[tag] = os.path.join(
                            os.path.relpath(
                                output_dir,
                                os.path.split(outfile)[0]),
                            "resources", link)

                html = str(soup)

            # Write page
            with codecs.open(outfile, encoding='utf-8', mode='w') as f:
                f.write(html)
                written_pages.append(filename)

            # Contains page listing?
            if re.search("(^<p>\[(!?)PAGES\s?(.*)\]</p>$)", html,
                         re.MULTILINE) is not None:
                if not filename in lastupdate["pagelisting_files"]:
                    lastupdate["pagelisting_files"].append(filename)
                outfiles.append(outfile)
                written_files[outfile]['pagelisting'] = True

            # Contains breadcrumb menu?
            if re.search("(^<p>\[(BREADCRUMB)\]</p>$)", html,
                         re.MULTILINE) is not None:
                written_files[outfile]['breadrumb'] = True

        # Substitute [PAGES]
        for outfile in [k for k,v in written_files.items() if v['pagelisting']]:
            with open(outfile) as f:
                content = f.read()
                pages = []  # [filename, metas]
                htmldir = os.path.split(outfile)[0]
                for directory in os.listdir(htmldir):
                    if directory == "resources":
                        continue
                    directory = os.path.abspath(os.path.join(htmldir,
                                                             directory))
                    if os.path.isdir(os.path.abspath(directory)):
                        page = os.path.join(directory, "index.html")
                        try:
                            pages.append([directory,
                                          written_files[page]['metas']])
                        except:
                            pages.append([directory, {}])
                for match in re.findall("(^<p>\[(!?)PAGES\s?(.*)\]</p>$)",
                                        content, re.MULTILINE):
                    pages.sort(key=lambda x: x[0])  # sort by filename
                    try:  # sort by meta
                        pages.sort(key=lambda x: x[1][match[2]])
                    except:
                        pass
                    if match[1] == '!':  # reverse sort
                        pages.reverse()
                    pages_list = '<ul class="pagelisting">\n'
                    for page in pages:
                        try:
                            item = '<li>' + pagelisting_format + '</li>'
                            p = re.compile(r"{{(.*?)}}")
                            for m in p.findall(pagelisting_format):
                                if m.lower() in page[1]:
                                    item = item.replace("{{" + m + "}}",
                                                        page[1][m.lower()])
                        except:
                            item = '<li><a href="{0}">{0}</a></li>'

                        pages_list += item.format(page[0]) + '\n'

                    pages_list += "</ul>\n"
                    content = content.replace("{0}\n".format(match[0]),
                                                             pages_list, 1)

            with open(outfile, 'w') as f:
                f.write(content)

        # Substitute [BREADCRUMB]
        for outfile in [k for k,v in written_files.items() if v['breadcrumb']]:
            with open(outfile) as f:
                content = f.read()
                if os.path.isabs(_metas["output_dir"]):
                    output_dir = _metas["output_dir"]
                else:
                    output_dir = os.path.abspath(
                        os.path.join(self.markdown_dir, _metas["output_dir"]))
                path = os.path.abspath(os.path.relpath(
                    os.path.split(outfile)[-1], output_dir))
                trail = []
                while path != output_dir:
                    path = os.path.split(path)[-1]
                    with open(os.path.join(path, 'index.html')) as f:
                        soup = BeautifulSoup(f.read(), "html.parser")
                        trail.append(soup.title.string)
                trail.reverse()
                for match in re.findall("(^<p>\[(BREADCRUMB)\]</p>$)",
                                        content, re.MULTILINE):
                    bc = '<span class="breadcrumb">~<span>/</span>'
                    for c,t in enumerate(trail):
                        bc += '<a href="{0}">{1}</a><span>/</span>'.format(
                            (c + 1) * '../', t)
                    bc += '</span>'
                    content = content.replace("{0}\n".format(match[0]), bc, 1)

            with open(outfile, 'w') as f:
                f.write(content)

        # Save update information
        lastupdate["time"] = time.time()
        try:
            with open(os.path.join(self.markdown_dir,
                                   ".lastupdate"), 'w') as f:
                json.dump(lastupdate, f)
        except:
            pass

        if written_files != {}:
            print("")
            for page in written_files:
                print(page)
            print("")
